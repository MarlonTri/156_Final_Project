{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tensorflow.keras.losses import MeanAbsolutePercentageError as MAPE\n",
    "from random import choice,random\n",
    "import tensorflow as tf\n",
    "from pprint import pformat,pprint\n",
    "import keras\n",
    "import pickle\n",
    "from keras import layers, models, regularizers, backend, utils\n",
    "import keras.backend.tensorflow_backend as tfback\n",
    "from IPython.display import display, clear_output\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression,Ridge\n",
    "from IPython.core.display import display, HTML\n",
    "from tqdm.keras import TqdmCallback\n",
    "\n",
    "#widen the container width if you want\n",
    "#display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "\n",
    "K = keras.backend\n",
    "mape = MAPE()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the nvidia-docker container supplied GPU and the tf is using it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the select data Data. This splits the data up into inputs and outputs, and normalizes both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = \"Sydney Tasmania Perth Adelaide\".split()\n",
    "\n",
    "xs_tests = dict()\n",
    "ys_tests = dict()\n",
    "xs_trains = dict()\n",
    "ys_trains = dict()\n",
    "for file in files:\n",
    "    csvfile = open(f\"/notebooks/WECs_DataSet/{file}_Data.csv\")    \n",
    "    reader = csv.reader(csvfile, delimiter = ',')\n",
    "    rows = [[float(v) for v in row] for row in reader]\n",
    "    rows = np.asarray(rows, dtype='float32')\n",
    "    xs = rows[:,0:32]\n",
    "    ys = rows[:,32:49]\n",
    "    ys = ys / np.max(ys)\n",
    "    xs = xs / np.max(xs)\n",
    "    xs_train, xs_test, ys_train, ys_test = train_test_split(xs,ys,test_size = 0.2,random_state=42)\n",
    "    xs_tests[file] = xs_test\n",
    "    ys_tests[file] = ys_test\n",
    "    xs_trains[file] = xs_train\n",
    "    ys_trains[file] = ys_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a model, this function gets the MAPE(Mean Absolute Percentage Error) of the model using test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error(file,model):\n",
    "    ys_predictions = model.predict(xs_tests[file])\n",
    "    mape = tf.keras.losses.MeanAbsolutePercentageError()\n",
    "    error = mape(ys_predictions, ys_tests[file]).numpy()\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(file,patience=100,verbose=0):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(1024, input_dim=32, activation = \"relu\"))\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    model.add(layers.Dense(2048, activation = \"relu\"))\n",
    "    model.add(layers.Dense(1024, activation = \"relu\"))\n",
    "    model.add(layers.Dense(17, activation = 'linear'))\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=patience)\n",
    "    model.compile(keras.optimizers.Adam(learning_rate=0.0003),loss=\"mean_absolute_percentage_error\")\n",
    "    loss_hist = model.fit(xs_trains[file], ys_trains[file], epochs = 15000, shuffle=True,\n",
    "                          verbose=0, validation_data = (xs_tests[file], ys_tests[file]), \n",
    "                          batch_size=2048, callbacks=[callback,TqdmCallback(verbose=verbose)])\n",
    "    return model,loss_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model and the history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sydney\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06ad9f1b18f64e70b6e8d7399b203525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linear Model MAPE:\t 3.2048799991607666\n",
      "Sequential Model MAPE:\t 1.2961041927337646\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tasmania\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "340f7d4806cb47cfaf4d53adf63cf888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linear Model MAPE:\t 6.566184043884277\n",
      "Sequential Model MAPE:\t 5.396560192108154\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Perth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac94ce40baad438a9538353f8e5440c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linear Model MAPE:\t 6.818752288818359\n",
      "Sequential Model MAPE:\t 5.166650295257568\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Adelaide\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3892abf956f2440b8a2863cd09e4aaf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linear Model MAPE:\t 6.791271686553955\n",
      "Sequential Model MAPE:\t 4.96780252456665\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for file in files:\n",
    "    print(file)\n",
    "    model,loss_hist = train_model(file,patience=100,verbose=1)\n",
    "    model.save(f\"models/{file}_model.h5\")\n",
    "    with open(f\"models/{file}_hist.pickle\",\"wb\") as f:\n",
    "        pickle.dump(loss_hist,f)\n",
    "    linear_model = LinearRegression().fit(xs_trains[file],ys_trains[file])\n",
    "\n",
    "    print(f\"Linear Model MAPE:\\t {get_error(file,linear_model)}\")\n",
    "    print(f\"Sequential Model MAPE:\\t {get_error(file,model)}\")\n",
    "    print(\"\\n\"*5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
